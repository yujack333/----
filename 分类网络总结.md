## Lenet
- 1.lenet确定了卷积神经网络的基本架构，卷积层、pooling层和全连接层

## Alexnet
这个网络做出了2点改进：
- 1.激活函数方面由之前的sigmoid函数改为了relu
  relu激活函数可以防止神经网络在训练时出现的梯度爆炸和消失的现象，加速训练过程。
- 2.引入了防止过拟合提高模型泛化能力的方法dropout
  dropout方法：在训练过程中随机的将一些神经元的输出设为0，即不参加前向和后向的传导。这样使得每次训练的网络都不一样，可以防止过拟合，提高模型的泛化能力。
- 其它：
引入局部归一化LRN，加速训练。
有重合的pooling。

## VGG
这个网络解释了两个问题：
- 1.小尺寸的卷积核要比大尺寸的卷积核要好
  3个3*3的卷积核操作之后的感受野和1个7*7的卷积核的感受野相同，但是参数更少（3×3<sup>2</sup>×C<sup>2</sup>=27×C<sup>2</sup>,7<sup>2</sup>×C<sup>2</sup>=49×C<sup>2</sup>）,提供的非线性性也更多(经过了三个非线性变换)。
- 2.网络越深精度越高
  这个是由实验结果得到的。VGG团队设计了由浅到深的5个网络，证明网络越深，表现越好。
- 其它：
参数量巨大，主要参数集中在全连接层。

## inception-v1
fire together,wire together.
大规模的神经网络，可以通过逐层聚类来得到一个最优的网络。臃肿的稀疏网络可以被不是性能的简化。
v1有多维度，1×1,3×3,5×5.

## inception-v2(batch-normalization)
- 问题：
BN要解决的的问题是Internal Covariate Shift.在训练过程中，参数的改变会导致下一层输入的分布的改变。而且网络越深，这个现象越明显。所以要用很小的学习率和精细的初始化，这样使得训练速度太慢。
- 方法：
分为两步：1.归一化 2.线性变换
1.归一化是为了解决上述的问题
2.线性变换是为了解决归一化后信息的改变。
原文章BN是在激活函数之前。BN(WX+b)
而有实验表明，放在激活函数之后效果更好一些。BN(X)
- 其它：
1.这个网络中还提出了用两个3×3换5×5
2.提出了label smooth

## inception-v3
这篇文章主要是对网络的建构进行了优化。
方法:
Factorization into small convolutions,主要思想就是将N×N的卷积转化为1×N和N×1两个卷积的结合。

## inception-v4
结合了resnet连接。

## resnet
- 问题：
论文首先抛出一个问题：
理论上来说：网络越深，网络的能力越强，精度应该也越高。但是实验表明，一个太深的网络的精度反而不如一个浅一点的网络。这个不是由于过拟合造成的，因为即使在训练集上的精度也比不过浅层网络。作者给出的解释是网络太深，梯度传导不下去，也就是说，是训练的问题。
- 方法：
加一个残差学习。
- 优势：
我觉得有两点优势：1.在前向传播的过程中低层的特征可以直接传向高层，与高层的特征进行融合。2.在后项传播的过程中，梯度可以通过残差连接更直接的传递到底层。

