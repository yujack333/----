#Lenet
1.lenet确定了卷积神经网络的基本架构，卷积层、pooling层和全连接层

#Alexnet
这个网络做出了2点改进：
1.激活函数方面由之前的sigmoid函数改为了relu
  relu激活函数可以防止神经网络在训练时出现的梯度爆炸和消失的现象，加速训练过程。
2.引入了防止过拟合提高模型泛化能力的方法dropout
  dropout方法：在训练过程中随机的将一些神经元的输出设为0，即不参加前向和后向的传导。这样使得每次训练的网络都不一样，可以防止过拟合，提高模型的泛化能力。
其它：
引入局部归一化LRN，加速训练。
有重合的pooling。

#VGG
这个网络解释了两个问题：
1.小尺寸的卷积核要比大尺寸的卷积核要好
  3个3*3的卷积核操作之后的感受野和1个7*7的卷积核的感受野相同，但是参数更少（3*3^2*C^2=27*C^2,7^2*C^2=49*C^2）,提供的非线性性也更多(经过了三个非线性变换)。
2.网络越深精度越高
  这个是由实验结果得到的。VGG团队设计了由浅到深的5个网络，证明网络越深，表现越好。
其它：
参数量巨大，主要参数集中在全连接层。

#inception-v1
