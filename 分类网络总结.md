## Lenet
- 1.lenet确定了卷积神经网络的基本架构，卷积层、pooling层和全连接层

## Alexnet
这个网络做出了2点改进：
- 1.激活函数方面由之前的sigmoid函数改为了relu
  relu激活函数可以防止神经网络在训练时出现的梯度爆炸和消失的现象，加速训练过程。
- 2.引入了防止过拟合提高模型泛化能力的方法dropout
  dropout方法：在训练过程中随机的将一些神经元的输出设为0，即不参加前向和后向的传导。这样使得每次训练的网络都不一样，可以防止过拟合，提高模型的泛化能力。降低模型的复杂程度来防止过拟合。
- 其它：
引入局部归一化LRN，加速训练。
有重合的pooling。

## VGG
这个网络解释了两个问题：
- 1.小尺寸的卷积核要比大尺寸的卷积核要好
  3个3*3的卷积核操作之后的感受野和1个7*7的卷积核的感受野相同，但是参数更少（3×3<sup>2</sup>×C<sup>2</sup>=27×C<sup>2</sup>,7<sup>2</sup>×C<sup>2</sup>=49×C<sup>2</sup>）,提供的非线性性也更多(经过了三个非线性变换)。
- 2.网络越深精度越高
  这个是由实验结果得到的。VGG团队设计了由浅到深的5个网络，证明网络越深，表现越好。
- 其它：
参数量巨大，主要参数集中在全连接层。

## inception-v1
fire together,wire together.
大规模的神经网络，可以通过逐层聚类来得到一个最优的网络。臃肿的稀疏网络可以被不是性能的简化。
v1有多维度，1×1,3×3,5×5.

## inception-v2(batch-normalization)
- 问题：
BN要解决的的问题是Internal Covariate Shift.在训练过程中，参数的改变会导致下一层输入的分布的改变。而且网络越深，这个现象越明显。所以要用很小的学习率和精细的初始化，这样使得训练速度太慢。
- 方法：
分为两步：1.归一化 2.线性变换
1.归一化是为了解决上述的问题
2.线性变换是为了解决归一化后信息的改变。
原文章BN是在激活函数之前。BN(WX+b)
而有实验表明，放在激活函数之后效果更好一些。BN(X)
- 其它：
1.这个网络中还提出了用两个3×3换5×5
2.提出了label smooth
- 作用：
防止过拟合，加速训练：消除不同batch之间的分布差异
## inception-v3
这篇文章主要是对网络的建构进行了优化。
方法:
Factorization into small convolutions,主要思想就是将N×N的卷积转化为1×N和N×1两个卷积的结合。

## inception-v4
结合了resnet连接。

## resnet
- 问题：
论文首先抛出一个问题：
理论上来说：网络越深，网络的能力越强，精度应该也越高。但是实验表明，一个太深的网络的精度反而不如一个浅一点的网络。这个不是由于过拟合造成的，因为即使在训练集上的精度也比不过浅层网络。作者给出的解释是网络太深，梯度传导不下去，也就是说，是训练的问题。
- 方法：
加一个残差学习。
- 优势：
我觉得有两点优势：1.在前向传播的过程中低层的特征可以直接传向高层，与高层的特征进行融合。2.在后项传播的过程中，梯度可以通过残差连接更直接的传递到底层。

## mobileNet-v1
- 问题：
在视觉任务中，深度学习方法往往计算量大、参数多。那么如何在精度不受太多的影响下减少计算量和参数量。
- 方法：
文章中提到一般的卷积操作有两个作用：1.利用卷积核计算特征，2.组合多个特征产生新的特征。文章从此处着手，将一般的卷积操作分解为两步，从而减少计算量和参数量。第一步：逐层卷积（depthwise convolutions）。对于输入的每一个通道都用一个卷积核来卷积。第二步：逐点的卷积。对前一步输出的特征利用1×1的卷积来对每一点进行卷积操作。这样两步操作就完成了卷积操作的两个作用，并且减少了计算量和参数量。具体的操作如下图：
![](/picture/MobileNet.png)
参数量图：

![](/picture/MobileNet_1.png)

一般卷积的参数量为：
![](https://latex.codecogs.com/gif.latex?D_k%5Ctimes%20D_k%5Ctimes%20M%20%5Ctimes%20N%5Ctimes%20D_f%5Ctimes%20D_f)；参数分别为卷积核大小、输入num_channel、输出num_channel、输出特征图大小

分解后的卷积参数量为：
![](https://latex.codecogs.com/gif.latex?D_k%5Ctimes%20D_k%5Ctimes%20M%5Ctimes%20D_f%5Ctimes%20D_f&plus;N%5Ctimes%20M%5Ctimes%20D_f%5Ctimes%20D_f)

值得注意的是：虽然在计算量和参数量上都有减少，但是在训练时所占的（显存/样本）会增加，应为每一层要存的输出变多了。
